{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed FWI with Devito using Tensorflow and Dask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inversion Computational Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing Google Cloud Storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket fwi-data created\n"
     ]
    }
   ],
   "source": [
    "import cloudpickle as pickle\n",
    "from google.cloud import storage\n",
    "\n",
    "saved_true_model_file = \"true_model\"\n",
    "saved_smooth_model_file = \"smooth_model\"\n",
    "saved_shot_data_file_prefix = \"shot\"\n",
    "bucket_name = \"fwi-data\"\n",
    "\n",
    "\n",
    "def create_bucket(bucket_name=bucket_name):\n",
    "    \"\"\"Creates a new bucket.\"\"\"\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.lookup_bucket(bucket_name)\n",
    "    if not bucket: \n",
    "        bucket = storage_client.create_bucket(bucket_name)\n",
    "    print('Bucket {} created'.format(bucket.name))\n",
    "    \n",
    "def delete_bucket(bucket_name=bucket_name):\n",
    "    \"\"\"Deletes a bucket. The bucket must be empty.\"\"\"\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.get_bucket(bucket_name)\n",
    "    if bucket: \n",
    "        bucket.delete()\n",
    "    print('Bucket {} deleted'.format(bucket.name))\n",
    "\n",
    "def upload_blob(bucket_name, source_file_name, destination_blob_name):\n",
    "    \"\"\"Uploads a file to the bucket.\"\"\"\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.get_bucket(bucket_name)\n",
    "    blob = bucket.blob(destination_blob_name)\n",
    "\n",
    "    blob.upload_from_filename(source_file_name)\n",
    "\n",
    "    print('File {} uploaded to {}.'.format(\n",
    "        source_file_name,\n",
    "        destination_blob_name))\n",
    "    \n",
    "def download_blob(bucket_name, source_blob_name, destination_file_name):\n",
    "    \"\"\"Downloads a blob from the bucket.\"\"\"\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.get_bucket(bucket_name)\n",
    "    blob = bucket.blob(source_blob_name)\n",
    "\n",
    "    blob.download_to_filename(destination_file_name)\n",
    "\n",
    "    print('Blob {} downloaded to {}.'.format(\n",
    "        source_blob_name,\n",
    "        destination_file_name))\n",
    "\n",
    "def dump_model(model, file_name):\n",
    "    pickle.dump(model, open(file_name, 'wb'))\n",
    "\n",
    "\n",
    "def get_model(file_name):\n",
    "    return pickle.load(open(file_name, \"rb\"))\n",
    "        \n",
    "\n",
    "def dump_shot_data(file_name, shot_id, src, true_data):\n",
    "    pickle.dump({'src': src, 'rec': true_data}, open(file_name, 'wb'))\n",
    "\n",
    "\n",
    "def get_shot_data(file_name, shot_id, dt):\n",
    "    shot_data = pickle.load(open(file_name, 'rb'))\n",
    "    shot_data['src'] = shot_data['src'].resample(dt)\n",
    "    shot_data['rec'] = shot_data['rec'].resample(dt)\n",
    "    return shot_data\n",
    "\n",
    "\n",
    "def upload_model_to_gcloud(model, model_file):\n",
    "    dump_model(model, model_file)\n",
    "    upload_blob(bucket_name, model_file, model_file)\n",
    "\n",
    "    \n",
    "def download_model_from_gcloud(model_file):\n",
    "    download_blob(bucket_name, model_file, model_file)\n",
    "    return get_model(model_file)\n",
    "\n",
    "\n",
    "def upload_shot_to_gcloud(shot_id, src, true_data):\n",
    "    file_name = '{}_{}'.format(saved_shot_data_file_prefix, shot_id)\n",
    "    dump_shot_data(file_name, shot_id, src, true_data)\n",
    "    upload_blob(bucket_name, file_name, file_name)\n",
    "\n",
    "    \n",
    "def download_shot_from_gcloud(shot_id, dt):\n",
    "    file_name = '{}_{}'.format(saved_shot_data_file_prefix, shot_id)\n",
    "    download_blob(bucket_name, file_name, file_name)\n",
    "    return get_shot_data(file_name, shot_id, dt)\n",
    "\n",
    "create_bucket()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up (synthetic) data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from examples.seismic import TimeAxis, RickerSource, Receiver\n",
    "from examples.seismic.acoustic import AcousticWaveSolver\n",
    "\n",
    "def generate_shot_data(t0, tn, f0, shots, receivers, client):\n",
    "    params = [t0, tn, f0, shots, receivers]\n",
    "    work = [params + [shot_id] for shot_id in range(shots)]\n",
    "\n",
    "    fgi = [client.submit(generate_shot_data_i, *job) for job in work]\n",
    "\n",
    "    wait(fgi)\n",
    "\n",
    "\n",
    "def generate_shot_data_i(t0, tn, f0, shots, receivers, shot_id):\n",
    "    true_model = download_model_from_gcloud(saved_true_model_file)\n",
    "\n",
    "    # Time step from model grid spacing\n",
    "    dt = true_model.critical_dt\n",
    "\n",
    "    # Acquisitional Geometry\n",
    "    time_range = TimeAxis(start=t0, stop=tn, step=dt)\n",
    "    src = RickerSource(name=\"src\", grid=true_model.grid, f0=f0,\n",
    "                       time_range=time_range)\n",
    "    \n",
    "    src.coordinates.data[0, :] = [30, shot_id*1000./(shots-1)]\n",
    "    \n",
    "    rec = Receiver(name=\"rec\", grid=true_model.grid, time_range=time_range,\n",
    "                   npoint=receivers)\n",
    "    rec.coordinates.data[:, 1] = np.linspace(0, true_model.domain_size[0],\n",
    "                                             num=receivers)\n",
    "    rec.coordinates.data[:, 0] = 980.  # 20m from the right end\n",
    "\n",
    "    # set up solver\n",
    "    solver = AcousticWaveSolver(true_model, src, rec, space_order=4)\n",
    "\n",
    "    # generate synthetic receiver data from true model\n",
    "    true_data, _, _ = solver.forward(src=src, m=true_model.m)\n",
    "    \n",
    "    upload_shot_to_gcloud(shot_id, src, true_data)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computational Consideration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shape = (101, 101)\n",
    "spacing = (10., 10.)\n",
    "origin = (0., 0.)\n",
    "\n",
    "shots = 9\n",
    "receivers = 101\n",
    "epochs = 5\n",
    "\n",
    "t0 = 0.\n",
    "tn = 1000.\n",
    "f0 = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### True and Smooth Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from examples.seismic import demo_model, plot_velocity, plot_perturbation\n",
    "from devito import configuration\n",
    "from distributed import Client, LocalCluster, wait\n",
    "\n",
    "configuration['log_level'] = 'WARNING'\n",
    "\n",
    "true_model = demo_model('circle-isotropic', vp=3.0, vp_background=2.5,\n",
    "                        origin=origin, shape=shape, spacing=spacing, nbpml=40)\n",
    "\n",
    "smooth_model = demo_model('circle-isotropic', vp=2.5, vp_background=2.5,\n",
    "                          origin=origin, shape=shape, spacing=spacing, nbpml=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dask Specifics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class fg_pair:\n",
    "    def __init__(self, f, g):\n",
    "        self.f = f\n",
    "        self.g = g\n",
    "\n",
    "    def __add__(self, other):\n",
    "        f = self.f + other.f\n",
    "        g = self.g + other.g\n",
    "        return fg_pair(f, g)\n",
    "\n",
    "    def __radd__(self, other):\n",
    "        if other == 0:\n",
    "            return self\n",
    "        else:\n",
    "            return self.__add__(other)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operators for gradient based inversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from devito import Function\n",
    "\n",
    "def fwi_gradient_i(shot_id):\n",
    "    from devito import clear_cache\n",
    "    \n",
    "    clear_cache()\n",
    "\n",
    "    smooth_model = download_model_from_gcloud(saved_smooth_model_file)\n",
    "\n",
    "    params = download_shot_from_gcloud(shot_id, smooth_model.critical_dt)\n",
    "    src = params['src']\n",
    "    rec = params['rec']\n",
    "    \n",
    "    solver = AcousticWaveSolver(smooth_model, src, rec, space_order=4)\n",
    "\n",
    "    smooth_data, u0, _ = solver.forward(src=src, m=smooth_model.m, save=True)\n",
    "\n",
    "    residual = Receiver(name='rec', grid=smooth_model.grid,\n",
    "                        time_range=rec.time_range,\n",
    "                        coordinates=rec.coordinates.data)\n",
    "\n",
    "    residual.data[:] = smooth_data.data[:] - rec.data[:]\n",
    "\n",
    "    f = .5*np.linalg.norm(residual.data.flatten())**2\n",
    "\n",
    "    grad = Function(name=\"grad\", grid=smooth_model.grid)\n",
    "    solver.gradient(rec=residual, u=u0, m=smooth_model.m, grad=grad)\n",
    "\n",
    "    g = np.array(grad.data[:])\n",
    "\n",
    "    return fg_pair(f, g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fwi_gradient(client, smooth_model, shots):\n",
    "    upload_model_to_gcloud(smooth_model, saved_smooth_model_file)\n",
    "\n",
    "    # Distribute job to workers - equivalent to the use of client.map(..)\n",
    "    fgi = [client.submit(fwi_gradient_i, shot_id) for shot_id in range(shots)]\n",
    "\n",
    "    # Distribute worklist to workers\n",
    "    total = client.submit(sum, fgi)\n",
    "    fg = total.result()\n",
    "\n",
    "    return fg.f, fg.g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FWI with Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import socket\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import functools as ft\n",
    "from dask.distributed import Client, LocalCluster, wait\n",
    "from dask_kubernetes import KubeCluster, make_pod_spec\n",
    "\n",
    "class TF_Devito_Fwi:\n",
    "\n",
    "    def __init__(self, nshots, nreceivers, true_model, smooth_model,\n",
    "                 t0, tn, f0, optimizer_func, hparams, workers=2):\n",
    "        # True and Smooth velocity models\n",
    "        upload_model_to_gcloud(true_model, saved_true_model_file)\n",
    "        self.smooth_model = smooth_model\n",
    "        true_data = true_model.m.data\n",
    "        \n",
    "        # Cluster setup\n",
    "        scheduler_ip = socket.gethostbyname(socket.gethostname())\n",
    "        scheduler_port = 30000\n",
    "        scheduler_address = str(scheduler_ip) + \":\" + str(scheduler_port)\n",
    "        cluster = KubeCluster.from_yaml('config/worker.yaml',\n",
    "                                        port=scheduler_port,\n",
    "                                        env={'DASK_SCHEDULER_ADDRESS': scheduler_address})\n",
    "        cluster.scale(nshots)\n",
    "        client = Client(cluster)\n",
    "        self.client = client\n",
    "        \"\"\"\n",
    "        cluster = LocalCluster(n_workers=workers, death_timeout=600)\n",
    "        client = Client(cluster)\n",
    "        \"\"\"\n",
    "        fwi_gradient_call = ft.partial(fwi_gradient, client, smooth_model, nshots)\n",
    "\n",
    "        generate_shot_data(t0, tn, f0, nshots, nreceivers, client)\n",
    "\n",
    "        # Create the tf graph\n",
    "        self.smooth_data = tf.Variable(self.smooth_model.m.data)\n",
    "        f, g = tf.py_func(fwi_gradient_call, [], [tf.float64, tf.float32])\n",
    "        if 'learning_rate' in list(hparams.keys()):\n",
    "            alpha = hparams['learning_rate']\n",
    "        else: \n",
    "            alpha = 0.005 / tf.reduce_max(g)\n",
    "        optimizer = optimizer_func(alpha, **hparams)\n",
    "        \n",
    "        self.train_op = optimizer.apply_gradients([(g, self.smooth_data)])\n",
    "        \n",
    "        self.relative_error = tf.norm((self.smooth_data-true_data)/true_data)\n",
    "        \n",
    "        self.sess = tf.Session()\n",
    "        self.sess.run(tf.global_variables_initializer())    \n",
    "        \n",
    "    def train(self, epochs):\n",
    "        for i in range(0, epochs):\n",
    "            _, re = self.sess.run([self.train_op, self.relative_error])\n",
    "            yield re\n",
    "        self.smooth_model.m.data[:] = self.smooth_data.eval(session=self.sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialise FWI process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File true_model uploaded to true_model.\n"
     ]
    },
    {
     "ename": "ApiException",
     "evalue": "(400)\nReason: Bad Request\nHTTP response headers: HTTPHeaderDict({'Audit-Id': 'bab67c18-3230-44cf-8858-723aa4269c66', 'Content-Type': 'application/json', 'Date': 'Mon, 08 Oct 2018 17:02:37 GMT', 'Content-Length': '1335'})\nHTTP response body: {\"kind\":\"Status\",\"apiVersion\":\"v1\",\"metadata\":{},\"status\":\"Failure\",\"message\":\"Pod in version \\\"v1\\\" cannot be handled as a Pod: v1.Pod: Spec: v1.PodSpec: Containers: []v1.Container: v1.Container: Resources: v1.ResourceRequirements: Limits: unmarshalerDecoder: quantities must match the regular expression '^([+-]?[0-9.]+)([eEinumkKMGTP]*[-+]?[0-9]*)$', parsing 665 ...IMIT_CPU)\\\"... at {\\\"kind\\\": \\\"Pod\\\", \\\"metadata\\\": {\\\"generateName\\\": \\\"dask-root-05be6f26-4\\\", \\\"labels\\\": {\\\"role\\\": \\\"worker\\\", \\\"dask.pydata.org/cluster-name\\\": \\\"dask-root-05be6f26-4\\\", \\\"user\\\": \\\"root\\\", \\\"app\\\": \\\"dask\\\", \\\"component\\\": \\\"dask-worker\\\"}, \\\"namespace\\\": \\\"default\\\"}, \\\"spec\\\": {\\\"containers\\\": [{\\\"args\\\": [\\\"dask-worker\\\", \\\"$(DASK_SCHEDULER_ADDRESS)\\\", \\\"--nthreads\\\", \\\"1\\\", \\\"--no-bokeh\\\", \\\"--memory-limit\\\", \\\"1GB\\\", \\\"--death-timeout\\\", \\\"60\\\"], \\\"env\\\": [{\\\"name\\\": \\\"DASK_SCHEDULER_ADDRESS\\\", \\\"value\\\": \\\"tcp://10.8.1.5:30000\\\"}, {\\\"name\\\": \\\"DASK_SCHEDULER_ADDRESS\\\", \\\"value\\\": \\\"10.8.1.5:30000\\\"}], \\\"image\\\": \\\"$(WORKER_IMAGE)\\\", \\\"imagePullPolicy\\\": \\\"Always\\\", \\\"name\\\": \\\"fwi-shot\\\", \\\"resources\\\": {\\\"limits\\\": {\\\"cpu\\\": \\\"$(LIMIT_CPU)\\\", \\\"memory\\\": \\\"$(LIMIT_MEMORY)\\\"}, \\\"requests\\\": {\\\"cpu\\\": \\\"$(REQUEST_CPU)\\\", \\\"memory\\\": \\\"$(REQUEST_MEMORY)\\\"}}}], \\\"restartPolicy\\\": \\\"Never\\\"}}\",\"reason\":\"BadRequest\",\"code\":400}\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mApiException\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-e608944012b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m fwi = TF_Devito_Fwi(shots, receivers, true_model, smooth_model,\n\u001b[0;32m----> 6\u001b[0;31m                     t0, tn, f0, optimizer, hparam, 5)\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mrelative_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-d6cbda0d2d21>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, nshots, nreceivers, true_model, smooth_model, t0, tn, f0, optimizer_func, hparams, workers)\u001b[0m\n\u001b[1;32m     22\u001b[0m                                         \u001b[0mport\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscheduler_port\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m                                         env={'DASK_SCHEDULER_ADDRESS': scheduler_address})\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mcluster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnshots\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mclient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcluster\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/venv/lib/python3.6/site-packages/dask_kubernetes/core.py\u001b[0m in \u001b[0;36mscale\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    352\u001b[0m         \u001b[0mpods\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cleanup_terminated_pods\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpods\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpods\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale_up\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpods\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpods\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m             \u001b[0mn_to_delete\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpods\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/venv/lib/python3.6/site-packages/dask_kubernetes/core.py\u001b[0m in \u001b[0;36mscale_up\u001b[0;34m(self, n, pods, **kwargs)\u001b[0m\n\u001b[1;32m    433\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_create\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m                     new_pods.append(self.core_api.create_namespaced_pod(\n\u001b[0;32m--> 435\u001b[0;31m                         self.namespace, self.pod_template))\n\u001b[0m\u001b[1;32m    436\u001b[0m                     \u001b[0mto_create\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/venv/lib/python3.6/site-packages/kubernetes/client/apis/core_v1_api.py\u001b[0m in \u001b[0;36mcreate_namespaced_pod\u001b[0;34m(self, namespace, body, **kwargs)\u001b[0m\n\u001b[1;32m   6055\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_namespaced_pod_with_http_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnamespace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6056\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6057\u001b[0;31m             \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_namespaced_pod_with_http_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnamespace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6058\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6059\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/venv/lib/python3.6/site-packages/kubernetes/client/apis/core_v1_api.py\u001b[0m in \u001b[0;36mcreate_namespaced_pod_with_http_info\u001b[0;34m(self, namespace, body, **kwargs)\u001b[0m\n\u001b[1;32m   6140\u001b[0m                                         \u001b[0m_preload_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_preload_content'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6141\u001b[0m                                         \u001b[0m_request_timeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_request_timeout'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6142\u001b[0;31m                                         collection_formats=collection_formats)\n\u001b[0m\u001b[1;32m   6143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6144\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcreate_namespaced_pod_binding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnamespace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/venv/lib/python3.6/site-packages/kubernetes/client/api_client.py\u001b[0m in \u001b[0;36mcall_api\u001b[0;34m(self, resource_path, method, path_params, query_params, header_params, body, post_params, files, response_type, auth_settings, async_req, _return_http_data_only, collection_formats, _preload_content, _request_timeout)\u001b[0m\n\u001b[1;32m    319\u001b[0m                                    \u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpost_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m                                    \u001b[0mresponse_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauth_settings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m                                    _return_http_data_only, collection_formats, _preload_content, _request_timeout)\n\u001b[0m\u001b[1;32m    322\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m             thread = self.pool.apply_async(self.__call_api, (resource_path, method,\n",
      "\u001b[0;32m/venv/lib/python3.6/site-packages/kubernetes/client/api_client.py\u001b[0m in \u001b[0;36m__call_api\u001b[0;34m(self, resource_path, method, path_params, query_params, header_params, body, post_params, files, response_type, auth_settings, _return_http_data_only, collection_formats, _preload_content, _request_timeout)\u001b[0m\n\u001b[1;32m    153\u001b[0m                                      \u001b[0mpost_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpost_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m                                      \u001b[0m_preload_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_preload_content\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m                                      _request_timeout=_request_timeout)\n\u001b[0m\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/venv/lib/python3.6/site-packages/kubernetes/client/api_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, query_params, headers, post_params, body, _preload_content, _request_timeout)\u001b[0m\n\u001b[1;32m    362\u001b[0m                                          \u001b[0m_preload_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_preload_content\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m                                          \u001b[0m_request_timeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_request_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m                                          body=body)\n\u001b[0m\u001b[1;32m    365\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"PUT\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m             return self.rest_client.PUT(url,\n",
      "\u001b[0;32m/venv/lib/python3.6/site-packages/kubernetes/client/rest.py\u001b[0m in \u001b[0;36mPOST\u001b[0;34m(self, url, headers, query_params, post_params, body, _preload_content, _request_timeout)\u001b[0m\n\u001b[1;32m    264\u001b[0m                             \u001b[0m_preload_content\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_preload_content\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m                             \u001b[0m_request_timeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_request_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m                             body=body)\n\u001b[0m\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m     def PUT(self, url, headers=None, query_params=None, post_params=None, body=None, _preload_content=True,\n",
      "\u001b[0;32m/venv/lib/python3.6/site-packages/kubernetes/client/rest.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, query_params, headers, body, post_params, _preload_content, _request_timeout)\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;36m200\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m299\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mApiException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_resp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mApiException\u001b[0m: (400)\nReason: Bad Request\nHTTP response headers: HTTPHeaderDict({'Audit-Id': 'bab67c18-3230-44cf-8858-723aa4269c66', 'Content-Type': 'application/json', 'Date': 'Mon, 08 Oct 2018 17:02:37 GMT', 'Content-Length': '1335'})\nHTTP response body: {\"kind\":\"Status\",\"apiVersion\":\"v1\",\"metadata\":{},\"status\":\"Failure\",\"message\":\"Pod in version \\\"v1\\\" cannot be handled as a Pod: v1.Pod: Spec: v1.PodSpec: Containers: []v1.Container: v1.Container: Resources: v1.ResourceRequirements: Limits: unmarshalerDecoder: quantities must match the regular expression '^([+-]?[0-9.]+)([eEinumkKMGTP]*[-+]?[0-9]*)$', parsing 665 ...IMIT_CPU)\\\"... at {\\\"kind\\\": \\\"Pod\\\", \\\"metadata\\\": {\\\"generateName\\\": \\\"dask-root-05be6f26-4\\\", \\\"labels\\\": {\\\"role\\\": \\\"worker\\\", \\\"dask.pydata.org/cluster-name\\\": \\\"dask-root-05be6f26-4\\\", \\\"user\\\": \\\"root\\\", \\\"app\\\": \\\"dask\\\", \\\"component\\\": \\\"dask-worker\\\"}, \\\"namespace\\\": \\\"default\\\"}, \\\"spec\\\": {\\\"containers\\\": [{\\\"args\\\": [\\\"dask-worker\\\", \\\"$(DASK_SCHEDULER_ADDRESS)\\\", \\\"--nthreads\\\", \\\"1\\\", \\\"--no-bokeh\\\", \\\"--memory-limit\\\", \\\"1GB\\\", \\\"--death-timeout\\\", \\\"60\\\"], \\\"env\\\": [{\\\"name\\\": \\\"DASK_SCHEDULER_ADDRESS\\\", \\\"value\\\": \\\"tcp://10.8.1.5:30000\\\"}, {\\\"name\\\": \\\"DASK_SCHEDULER_ADDRESS\\\", \\\"value\\\": \\\"10.8.1.5:30000\\\"}], \\\"image\\\": \\\"$(WORKER_IMAGE)\\\", \\\"imagePullPolicy\\\": \\\"Always\\\", \\\"name\\\": \\\"fwi-shot\\\", \\\"resources\\\": {\\\"limits\\\": {\\\"cpu\\\": \\\"$(LIMIT_CPU)\\\", \\\"memory\\\": \\\"$(LIMIT_MEMORY)\\\"}, \\\"requests\\\": {\\\"cpu\\\": \\\"$(REQUEST_CPU)\\\", \\\"memory\\\": \\\"$(REQUEST_MEMORY)\\\"}}}], \\\"restartPolicy\\\": \\\"Never\\\"}}\",\"reason\":\"BadRequest\",\"code\":400}\n\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.train.MomentumOptimizer\n",
    "\n",
    "hparam = {'momentum': 0.4, 'use_nesterov': True}\n",
    "\n",
    "fwi = TF_Devito_Fwi(shots, receivers, true_model, smooth_model,\n",
    "                    t0, tn, f0, optimizer, hparam, 5)\n",
    "\n",
    "relative_losses = np.zeros((epochs, 1))\n",
    "\n",
    "for i, rl in enumerate(fwi.train(epochs)):\n",
    "    print(\"Epoch: {}, Relative Losses: {}\".format(i, rl))\n",
    "    relative_losses[i] = rl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nbpml = true_model.nbpml\n",
    "smooth_model.vp = np.sqrt(1. / smooth_model.m.data[nbpml:-nbpml, nbpml:-nbpml])\n",
    "\n",
    "plot_velocity(smooth_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#NBVAL_SKIP\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot objective function decrease\n",
    "plt.figure()\n",
    "plt.loglog(relative_losses)\n",
    "plt.xlabel('Iteration number')\n",
    "plt.ylabel('True relative error')\n",
    "plt.title('Convergence')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
